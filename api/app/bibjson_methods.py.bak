# -*- coding: utf-8 -*-
#python2.7 -> python 3.7.2

import json
import re
import os.path
from collections import Counter


## FINAL GOAL = RETURN THIS
# hits = []


bibjson_aliases = {'abstract': ['abstract', 'AB'], 'address': ['address'], 'annote': ['annote', 'descr', 'desc', 'N2'], 'booktitle': ['booktitle'], 'chapter': ['chapter'], 'crossref': ['crossref'], 'edition': ['edition'], 'howpublished': ['howpublished'], 'institution': ['institution'], 'key': ['key'], 'month': ['month'], 'note': ['note'], 'number': ['number'], 'organization': ['organization'], 'pages': ['pages', 'page'], 'publisher': ['publisher', 'PB', 'publishers'], 'school': ['school'], 'series': ['series'], 'title': ['title', 'TI', 'titleInfo', 'Title'], 'type': ['type', 'TY', 'genre', 'ENTRYTYPE'], 'volume': ['volume', 'VL'], 'year': ['year', 'PY', 'DA', 'Year', 'publish_date'], 'author': ['author', 'AU', 'contributors', 'name', 'z_authors', 'Authors'], 'editor': ['editor'], 'identifier': ['doi', 'identifier', 'identifierwodash', 'md5', 'issn', 'isbn', 'ID', 'SN', 'DO', 'issne', 'issnp', 'identifiers', 'journal_issns', 'ISBN-13', 'olid', 'isbn-10'], 'link': ['locator', 'url', 'href', 'L1', 'UR', 'url_for_pdf', 'url_for_landing_page', 'best_oa_location', 'links'], 'subject': ['subject', 'KW', 'subjects'], 'journal': ['journal']}

extra_aliases = {'coverurl': ['coverurl', 'img_href'], 'language': ['language', 'LA', 'languages'], 'pagesinfile': ['pagesinfile'], 'tags': ['tags'], 'filetype': ['extension'], 'source': ['source'], 'rank': ['rank'], 'place_published': ['CY'], 'issue': ['IS', 'issue'], 'startpage': ['SP'], 'lastpage': ['EP']}

# careful = ['core']
#lvl2_aliases = {'core': {'name': {'affiliation': ['extra', 'affiliation']}, {'namePart':['bibjson', 'author']}},

#type_aliases = {'type': ['TY', 'genre', 'type']}

# careful_aliases = {"name": [{"path": "['namePart'][0][0]['text']", "condition": {"@type": "family"}, "in_nice_key": "name"}, {"path": "['namePart'][0][1]['#text']", "condition": {"@type": "given"}, "in_nice_key": "name"}], "originInfo": {"path": "['dateIssued']['#text']", "in_nice_key": "year"}, "relatedItem": [{"path": "['relatedItem']['part']['extent']['start']", "in_nice_key": "startpage"}, {"path": "['relatedItem']['part']['extent']['end']", "in_nice_key": "lastpage"}, {"path": "['relatedItem']['part']['detail']['number']", "in_nice_key": "issue"}, {"path": "['relatedItem']['identifier']['#text']", "in_nice_key": "identifier"}], "titleInfo": {"path": "['title']", "in_nice_key": "title"}}

fields = {'strings': ['abstract', 'address', 'annote', 'booktitle', 'chapter', 'crossref', 'edition', 'howpublished', 'institution', 'key', 'month', 'note', 'number', 'organization', 'pages', 'publisher', 'school', 'series', 'title', 'type', 'volume', 'year', 'journal', 'language'], 'list_of_dicts': ['author', 'editor', 'identifier', 'link', 'subject'], 'objects': ['']}

types = [{'article': {'required': ['author', 'title', 'journal', 'year', 'volume'], 'optional': ['number', 'pages', 'month', 'note', 'key'], 'aliases': ['article', 'jour']}}, {'book': {'required': ['author', 'editor', 'title', 'publisher', 'year'], 'optional': ['volume', 'number', 'series', 'address', 'edition', 'month', 'note', 'key', 'url', 'type'], 'aliases': ['book', 'edition', 'work']}}, {'booklet': {'required': ['title'], 'optional': ['author', 'howpublished', 'address', 'month', 'year', 'note', 'key'], 'aliases': ['booklet']}}, {'conference': {'required': ['author', 'editor', 'title', 'chapter/pages', 'publisher', 'year'], 'optional': ['volume', 'number', 'series', 'type', 'address', 'edition', 'month', 'note', 'key'], 'aliases': ['conference']}}, {'inbook': {'required': ['author', 'title', 'booktitle', 'publisher', 'year'], 'optional': ['editor', 'volume', 'number', 'series', 'type', 'chapter', 'pages', 'address', 'edition', 'month', 'note', 'key'], 'aliases': ['inbook']}}, {'incollection': {'required': ['author', 'title', 'booktitle', 'year'], 'optional': ['editor', 'volume', 'number', 'series', 'pages', 'address', 'month', 'organization', 'publisher', 'note', 'key'], 'aliases': ['incollection']}}, {'inproceedings': {'required': ['author', 'title', 'booktitle', 'year'], 'optional': ['editor', 'volume', 'number', 'series', 'pages', 'address', 'month', 'organization', 'publisher', 'note', 'key'], 'aliases': ['inproceedings', 'conference paper']}}, {'manual': {'required': ['title'], 'optional': ['author', 'organization', 'address', 'edition', 'month', 'year', 'note', 'key'], 'aliases': ['manual']}}, {'masterthesis': {'required': ['author', 'title', 'school', 'year'], 'optional': ['type', 'address', 'month', 'note', 'key'], 'aliases': ['masterthesis']}}, {'misc': {'required': ['none'], 'optional': ['author', 'title', 'howpublished', 'month', 'year', 'note', 'key'], 'aliases': ['misc', 'creative work', 'artwork']}}, {'phdthesis': {'required': ['author', 'title', 'school', 'year'], 'optional': ['type', 'address', 'month', 'note', 'key'], 'aliases': ['phdthesis']}}, {'proceedings': {'required': ['title', 'year'], 'optional': ['editor', 'volume', 'number', 'series', 'address', 'month', 'publisher', 'organization', 'note', 'key'], 'aliases': ['proceedings']}}, {'techreport': {'required': ['author', 'title', 'institution', 'year'], 'optional': ['type', 'number', 'address', 'month', 'note', 'key'], 'aliases': ['techreport']}}, {'unpublished': {'required': ['author', 'title', 'note'], 'optional': ['month', 'year', 'key'], 'aliases': ['unpublished']}}]

current_source = ''

resolving_nesting = [{'core': {'problem_fields': [{'field_name': 'title', 'depth_and_keys': 'title'}]}, 'aaaaarg': {'problem_fields': [{'field_name': 'language', 'depth_and_keys': 'key'}]}}]

sources_bibjson = [{'doaj': [{'level': 'hits','depth': 'results'}, {'level': 'hit', 'depth': 'bibjson'}]}]


#for additional metadata?
def crossref_q():
    pass


def join_lists(slovar):
    list = []
    for key, value in slovar.iteritems():
        if len(value)>1:
            list.append(value)
        else:
            pass
    return list

def join_lists2(slovar):
    oput = []
    for k, v in slovar.iteritems():
        for xx in v:
            oput.append(xx)
    return oput

def dis_ambig(slovar, alias):
    for key, value in slovar.iteritems():
        if alias in value:
            return key

def recognize_isbn(string):
    pass

###STRINGS // not really necessary
def make_string(fieldname, fielddata):
    #print "hey this is make string meeking "+fieldname+" out of "+fielddata
    return dict(fieldname=fielddata)


def gen_dict_extract(key, var):
    all_paths = []
    if hasattr(var,'iteritems'):
        for k, v in var.iteritems():
            if k == key:
                yield v
            if isinstance(v, dict):
                for result in gen_dict_extract(key, v):
                    yield result
            elif isinstance(v, list):
                for d in v:
                    for result in gen_dict_extract(key, d):
                        yield result

def get_all_keys(dict_item, key_base=''):
    all_paths = []
    if isinstance(dict_item, dict):
        for key in dict_item:
            if key_base:
                new_key = key_base + "/" + key
            else:
                new_key = key
            all_paths.extend(get_all_keys(dict_item[key], new_key))
    else:
        if key_base:
            all_paths.append(key_base)

    return all_paths

def really_get_all_keys(dict_item, key_base=''):
    all_paths = []
    if isinstance(dict_item, dict):
        #print "THIS IS DICT SIRS: ", dict_item
        for key in dict_item:
            if key_base:
                new_key = key_base + "/" + key
            else:
                new_key = key
            all_paths.extend(really_get_all_keys(dict_item[key], new_key))
    else:
        if key_base:
            all_paths.append(key_base)

    return all_paths


def dict_path(path,my_dict,out):
    for k,v in my_dict.iteritems():
        if isinstance(v,dict):
            dict_path(path+"['"+k+"']",v,out)
        elif isinstance(v,list):
            for blj in v:
                dict_path(path+"['"+k+"']",blj,out)
        else:
            #print path+"_"+k,"=>",v
            struct = {'path': path+"['"+k+"']", 'value': v}
            #struct = path+"_"+k
            out.append(struct)



def parse_keys(all_paths, search_key):
    output = []

    if all_paths:
        for i in all_paths:
            op = i.split('/')
            mar = [n for n in op if op[-1] == search_key]
            if mar:
                output.append(mar)

    return output


def return_data_type(data_value):
    if isinstance(data_value, dict):
        return "dict"
    elif isinstance(data_value, list):
        return "list"
    elif isinstance(data_value, basestring):
        return "string"
    else:
        return None

def resolve_depth(pod, source, level):
    depth = ''
    for i in pod:
        #print i
        if source in i:
            for k, v in i.iteritems():
                #print k, v
                for s in v:
                    #print s
                    if s['level'] == level:
                        depth = s['depth']
    return depth

def get_it(lod, val):
    for i in lod:
        if val in i:
            return True
        else:
            return False

###LISTS OF DICTS
def list_of_dicts(fieldname, fielddata, original_key, source):

    #print fieldname, fielddata
    output = []

    #core settings?

    #disassemble fielddata?
    if type(fielddata) == list:
        nice_input = fielddata
    elif type(fielddata) == dict:
        # print "DICT!"
        # print "CURRENT SOURCE", current_source
        # if current_source == "core":
        #     print fielddata['affiliation']
        #     nice_input = ''
        #     for i in fielddata['namePart']:
        #         print i["#text"]
        #         nice_input = nice_input+' '+i['#text']
        #     print nice_input
        #     #print fielddata['namePart']
        #     print fielddata['role']
        # else:
        #     pass
        pass
    else:
        if ";" in fielddata:
            try:
                nice_input = fielddata.split(';')
                #print nice_input
            except:
                pass
        else:
            try:
                nice_input = fielddata.split(',')
            except:
                pass

    if fieldname == "author":
        # print "yesyes author"
        # THIS IS WHERE CORE GETS STUCK
        #print "DATA: ", fielddata
        #print "\n\n"

        # FIX THIS THIS NEEDS TO BE ONE OF THE PARAMETERS
        #source = "core"

        def extraction(fielddata):
            one =  {"name": "", "alternate": [""],  "firstname": "", "lastname": "", "type": "author"}
            #role = fielddata["role"]["roleTerm"]["#text"]
            #affiliation = fielddata["affiliation"]
            #print role
            #print affiliation

            for h in fielddata["namePart"]:
                #print h
                if h["@type"] == "given":
                    fname = h["#text"]
                    one["firstname"] = fname
                elif h["@type"] == "family":
                    lname = h["#text"]
                    one["lastname"] = lname
                else:
                    pass

                try:
                    one["name"] = fname+" "+lname
                except:
                    pass
            #print one
            return one


        #this is ok, but then there's multiple authors

        if source == "core":
            #checkf = [n for n in fielddata["namePart"]]
            if isinstance(fielddata, dict):
                one = extraction(fielddata)
                output.append(one)

            elif (fielddata, list):
                #print "SUMLJIVI: ", fielddata
                for i in fielddata:
                    try:
                        one = extraction(i, source)
                        #print "THE OTHER ONE: ", one
                        output.append(one)
                    except TypeError:
                        pass
            else:
                pass
            #print signs
            # if len(checkf) == 2:

            # elif len(checkf) > 1:
            #     print "LOONGER THAN ONE"
        elif source == "oadoi":
            one =  {"name": "", "alternate": [""],  "firstname": "", "lastname": ""}
            if isinstance(fielddata, list):
                #print "SUMLJIVI: ", fielddata
                print "LIST ", fielddata
                for i in fielddata:
                    try:
                        one['firstname'] = i['given']
                        one['lastname'] = i['family']
                        one["name"] = i['given']+" "+i['family']

                        output.append(one)
                    except TypeError:
                        pass
            else:
                pass

        else:
            try:
                for i in nice_input:
                    one =  {"name": "", "alternate": [""],  "firstname": "", "lastname": "", "type": "author"}

                    # this works for aarg let's hope it doesn't ruin anything elsewhere

                    ftype = return_data_type(i)
                    #print ftype
                    if ftype == 'dict':
                        ks = get_all_keys(i)
                        tmp = gen_dict_extract(ks[0], i)
                        lst = [el for el in tmp]
                        one['name'] = lst[0]
                        output.append(one)
                    else:
                        one['name'] = i
                        output.append(one)

            except:
                pass

        #print "OOOOooooOOOOO: ", output

    elif fieldname == "editor":
        #print "yesyes editor"
        try:
            for i in nice_input:
                one =  {"name": "", "alternate": [""],  "firstname": "", "lastname": "", "type": "editor"}
                one['name'] = i
                output.append(one)
        except:
            pass

    elif fieldname == "identifier":
        #print "yesyes identifier", fielddata
        #we need a tester here? a regex tester?
            # recognize DOI and ISBN
            # split if contains space slash or comma?
        def appendix(input_lst, keyname=original_key):
            for i in input_lst:
                #print "IDENTIFIER", i
                one = {"id": "", "type": ""}
                one['type'] = keyname
                one['id'] = i
                #print one
                output.append(one)

        def appendone(input_val, keyname=original_key):
                one = {"id": "", "type": ""}
                one['type'] = keyname
                one['id'] = input_val
                #print one
                output.append(one)

        #print "IDENTIFIER", fielddata, original_key
        if type(fielddata) == list:
            br = ' '.join(fielddata)

            #print "BR!", br

            isbns = re.findall(r"\d{10,13}", br)

            # if isbns:
            #     print isbns

            dois = re.findall(r"/^10.\d{4,9}/[-._;()/:A-Z0-9]+$/i", br)
            #print "DOIS", dois

            # if dois:
            #     print dois
            if len(isbns)>1:
                appendix(isbns, "isbn")
            else:
                pass

            if len(dois)>1:
                appendix(dois, "doi")
            else:
                try:
                    #specifically because of osf ...
                    dois2 = re.findall(r"10.[-._;()/:A-Z0-9]+$", br)
                    #print "DOIS2: ", dois2
                    appendix(dois2, "doi")
                except:
                    pass
        elif type(fielddata) == dict:
            print fielddata

        else:
            appendone(fielddata)

        #print output




    elif fieldname == "link":
        #print "yesyes link"
        one = {"format": "", "url": ""}
        if type(fielddata)==list:
            #print "LISTLIST"

            if isinstance(fielddata[0], dict):
                for i in fielddata:
                    output.append(i)
            else:
                for i in fielddata:
                    try:
                        for key, value in i.iteritems():
                            #print "list_dict: ", key, value
                            one['format'] = key
                            one['url'] = value
                            #print one
                            output.append(one)
                    except:
                        #print "list_no_dict: ", i
                        anch = "href"
                        #print "trysplit", anch
                        one['format'] = anch
                        one['url'] = i
                        #print "one: ", one
                        output.append(one)
        elif type(fielddata) == dict:
            if source =="oadoi":
                print "OADOI"
                print fielddata
                one['url'] = fielddata['url_for_pdf']
                one['format'] = 'pdf'
                output.append(one)
        else:
            try:
                #anch = fielddata.rsplit('.', 1)[1]
                #print "no_list: ", fielddata
                anch = "href"
                #print "trysplit", anch
                one['anchor'] = anch
                one['url'] = fielddata
                output.append(one)
            except:
                pass

        #one['url'] = fielddata
        #print "output: ", output


        # for i in nice_input:
        #     one = {"anchor": "", "url": ""}
        #     try:
        #         anch = i.rsplit('.', 1)[1]
        #         #print "trysplit", anch
        #         one['anchor'] = anch
        #     except:
        #         pass
        #     one['url'] = i
        #     output.append(one)

    elif fieldname == "subject":
        for i in fielddata:
            output.append(i)
        # one = {}
        # print "yesyes subject"
        # #print fieldname, original_key, fielddata
        # one[fieldname] = []
        # for i in fielddata:
        #     one[fieldname].append(i)
        # print one['subject'][0]
        #
        # output.append(one)


    #print "output=",output
    #print output
    return output

    # author =  [{"name": "", "alternate": [""],  "firstname": "", "lastname": ""}]
    #
    # editor = [{"name": "", "alternate": [""],  "firstname": "", "lastname": ""}]
    #
    # identifier = [{"id": "", "type": ""}]
    #
    # link = [{"content_type":"", "type":"", "url":""}]
    #
    # subject = [{"code": "", "scheme": "", "term": ""}]

###NESTED DICTS
def make_a_journal(journaldata):
    #j_fields = {'issns': '', 'language': '', 'publisher': '', 'name': '', 'volume': '', 'pages': '', 'year': '', 'issue': ''}
    j_fields = ['issns', 'language', 'publisher', 'journal', 'volume', 'pages', 'year', 'issue', 'month']
    #print journaldata

    j_dict = {}
    out = {'bibjson': {}, 'extra': {}}

    for k, v in journaldata['bibjson'][0].iteritems():
        if k in j_fields:
            j_dict[k] = v
            #delete fields from dict!
        else:
            out['bibjson'][k] = v

    for k, v in journaldata['extra'][0].iteritems():
        if k in j_fields:
            j_dict[k] = v
            #delete fields from dict!
        else:
            out['extra'][k] = v

    out['bibjson']['journal'] = j_dict
    return out


def type_maker(type, data_key, data_value, source):
    bibjson = {}
    af2 = []
    for i in types:
        if type in i:
            #joins optional and required fields in one list
            af = join_lists(i[type])
            af2 = af[0]+af[1]
        else:
            pass
            #print af2

    #print "DATA VALUE", data_value
    #print "DATAKEY", data_key
    #print "TYPE ", type
    #print af2


    new_key = dis_ambig(bibjson_aliases, data_key)
    print "newkey", new_key
    # print "CURRENTSOURCE", current_source

    # resolving_nesting = [{'core': {'problem_fields': [{'field_name': 'title', 'depth_and_keys': 'title'}]}}]


    if new_key in fields['strings']:

        dv = return_data_type(data_value)

        if dv != 'string':
            for s in resolving_nesting:
                if source in s:
                    #print "elo!"
                    print s[source]
                    pf = s[source]['problem_fields']
                    for f in pf:
                        if f['field_name'] == new_key:
                            temp = data_value[f['depth_and_keys']]
                            print temp
                            data_value = temp
                else:
                    pass

        bibjson[new_key] = data_value



        # if new_key == 'title':
        #     print data_value

    elif new_key in fields['list_of_dicts']:
        #print new_key
        lod = list_of_dicts(new_key, data_value, data_key, source)
        #print "LOD: ",lod
        # if new_key == "identifier":
        #     print "IDENTIFIER", lod
        #
        # bibjson[new_key] = []
        #
        # for i in lod:
        #     if new_key in bibjson:
        #         bibjson[new_key].append(i)
        bibjson[new_key] = lod
    elif new_key in fields['objects']:
        bject = make_a_journal(new_key, data_value, data_key)

        # try:
        #     if bibjson[new_key]:
        #         for i in lod:
        #             bibjson[new_key].append(i)
        # except KeyError:
        #     bibjson[new_key] = lod
    #print bibjson
    return bibjson


#source = 'stuff/libgen_json.json'
#source = 'stuff/sample_hits.json'
#source = 'stuff/libg_article.json'
#source = 'stuff/sample_hits2.json'
#source = 'stuff/core_hits.json'
#source = 'stuff/osf_hits.json'
source ='stuff/arg_output.json'

def mein_main(incoming):
    hits = []
    print "init mein_main"
    print "initial hits: ", hits
    hit = {"bibjson": []}
    try:
        os.path.isfile(incoming)
        #print "FILE FILE!"
        f = open(incoming)
        data = json.load(f)
        data = data['entries']
    except TypeError:
        #print "NOFILE NOFILE"
        data = incoming['entries']

    for i in data:
        new_hits = []
        print "new hits: ", new_hits

        source = i.keys()[0]
        global current_source
        current_source = source

        if get_it(sources_bibjson, current_source):
            #print json.dumps(i, sort_keys=True, indent=4)
            cleaner = resolve_depth(sources_bibjson, current_source, 'hits')
            try:
                for x in i[source]['hits'][cleaner]:
                    new_hits.append(x)
            except TypeError:
                pass
        else:
            try:
                for x in i[source]['hits']:
                    new_hits.append(x)
            except TypeError:
                pass


        #rename all the compatible fields?
        ea = join_lists2(extra_aliases)
        bt = join_lists2(bibjson_aliases)
        #that's probably best

        # sources_bibjson = [{'core': [{'level': 'hits','depth': 'results'}, {'level': 'hit', 'depth': 'bibjson'}]}]
        #print json.dumps(i, sort_keys=True, indent=4)


        for i in new_hits:
            hit = {"bibjson": [], "extra": []}
            bibjson_output = {}
            extra_output = {}
            extra_fields = {}
            bibjson_fields = {}
            original_type = ''
            nice_type = ''
            past_keys = []

            cs = {"source": current_source}



            # WHAT DOES THIS DO?
            if get_it(sources_bibjson, current_source):
                count = 0

                cleaner = resolve_depth(sources_bibjson, current_source, 'hit')

                hit['bibjson'].append(i[cleaner])
                hit['extra'].append({'source': current_source})
                hit['extra'].append({'rank': count})

                hits.append(hit)
                count+=1
            else:
                if isinstance(i, dict):
                    for key, value in i.iteritems():
                        mua = {}
                        #print key, value

                        #listo = type_aliases['type']
                        listo = bibjson_aliases['type']
                        #print listo

                        #extracts original type value
                        if key in listo:
                            if isinstance(value, (list)):
                                original_type = value[0].lower()
                            elif isinstance(value, dict):
                                #print "ELO"
                                #print value['key']
                                #print next(iter(value))
                                tmp_typ = value[next(iter(value))]
                                try:
                                    original_type = tmp_typ.split('/')[-1]
                                    print original_type
                                except:
                                    original_type = value[next(iter(value))]
                                    #print "NOSLASH"
                            else:
                                original_type = value.lower()


                        #translates keys to standard values
                        if key in ea:
                            try:
                                extra_fields[dis_ambig(extra_aliases, key)] = value
                            except:
                                pass
                        elif key in bt:
                            bibjson_fields[key] = value
                        else:
                            pass
                else:
                    pass


                #print "TYPES ", types
                #print "BIBFIELDS ", bibjson_fields
                #print "EXFIELDS ",extra_fields
                print "\n\n"

                # standardizes type names based on aliases in types (top)
                for i in types:
                    type_name = i.keys()[0]
                    #print "TYPE NAME: ", type_name
                    alia = i[type_name]['aliases']
                    #print "OG", original_type
                    #print alia

                    if original_type in alia:
                        print "Original type in aliases."
                        nice_type = type_name
                    else:
                        pass


                #print nice_type

                #print bibjson_fields
                for key, value in bibjson_fields.iteritems():
                    #print key, value
                    #bibjson_one = type_maker(nice_type, key, value, allpaths)

                    # IF NICE TYPE IS MISSING ASSIGN misc
                    if nice_type == '':
                        nice_type = 'misc'


                    # USES CLEANED UP TYPE VLAUE IF TYPE FIELD IS BEING PROCESSED
                    if key == "type":
                        value = nice_type
                        #print "TYP!", value


                    bibjson_one = type_maker(nice_type, key, value, source)
                    #print "bibjson_one: ", bibjson_one
                    # past_keys.append()
                    #print bibjson_one
                    #print "past keys: ",past_keys


                    if bool(bibjson_one) == True:
                        #print "BIBJSON ONE: ", bibjson_one
                        for k, v in bibjson_one.iteritems():
                            #print k, v
                            #checks if value already exists in bibjson output ... if so it needs to be appended to existing bibjson differently
                            if k in past_keys:
                                if type(v)==list:
                                    try:
                                        #print "ALREADYIS",v
                                        for i in v:
                                            bibjson_output[k].append(i)
                                    except:
                                        pass
                                else:
                                    try:
                                        #print "ALREADYIS",v
                                        bibjson_output[k].append(v)
                                    except:
                                        pass

                            else:
                                try:
                                    #print "YETIZNT", v
                                    bibjson_output[k] = v
                                except:
                                    pass

                    if bibjson_one.keys()[0] not in past_keys:
                        past_keys.append(bibjson_one.keys()[0])

                    #print bibjson_output
                for key, data_value in extra_fields.iteritems():
                    #print source
                    # CHECKS IF STUFF IS WEIRDLY NESTED
                    for s in resolving_nesting:
                        if source in s:
                            pf = s[source]['problem_fields']
                            for f in pf:
                                if f['field_name'] == key:

                                    # this is not sustainable
                                    if return_data_type(data_value) == "list":
                                        data_value = data_value[0]

                                    temp = data_value[f['depth_and_keys']]
                                    #print temp
                                    data_value = temp
                        else:
                            pass

                    extra_output[key] = data_value

                #print "ONE OUTPUT", bibjson_output
                extra_output['source'] = current_source

                hit['bibjson'].append(bibjson_output)
                hit['extra'].append(extra_output)


                #CORRECTION IF IT'S A JOURNAL
                try:
                    if bibjson_output['type'] == 'article':
                        #print "WE ARE DEALING WITH A JOURNAL HERE!"
                        hit = make_a_journal(hit)
                        #hit['bibjson'].append(hit)
                except KeyError:
                    pass
                    #print "KEYERROR SRC:", bibjson_output


                #print json.dumps(hit, sort_keys=True, indent=4)

                hits.append(hit)

    #print json.dumps(hits, sort_keys=True, indent=4)
    # for i in hits:
    #     print i['bibjson'][0]['link']
    #print len(hits)
    #print hits
    #print json.dumps(hits, sort_keys=True, indent=4)
    return hits


# hits = mein_main(source)
# f = open("stuff/bibjson_methods_arg_output.json", "w+")
# f.write(json.dumps(hits, sort_keys=True, indent=4))
# f.close()
